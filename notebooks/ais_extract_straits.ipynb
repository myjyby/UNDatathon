{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7511b2ea-767a-4414-b0c7-575be061cdfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/sparkuser/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting boto3\n",
      "  Downloading boto3-1.28.78-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore<1.32.0,>=1.31.78 (from boto3)\n",
      "  Downloading botocore-1.31.78-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.8.0,>=0.7.0 (from boto3)\n",
      "  Downloading s3transfer-0.7.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.8/site-packages (from botocore<1.32.0,>=1.31.78->boto3) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.32.0,>=1.31.78->boto3) (1.26.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.78->boto3) (1.15.0)\n",
      "Downloading boto3-1.28.78-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.31.78-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m233.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading s3transfer-0.7.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m239.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.28.78 botocore-1.31.78 jmespath-1.0.1 s3transfer-0.7.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m21.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "842c1313-02f7-4c4a-a72f-06c6778195d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# necessary import functions\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from sedona.register import SedonaRegistrator\n",
    "SedonaRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5795be89-6fb9-4bf8-9e3e-e26f05689d11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "ACCESS_KEY = '' # Removed\n",
    "SECRET_KEY = '' # Removed\n",
    "\n",
    "def upload_file_to_s3(query, file_path, bucket_name):\n",
    "    df = query.toPandas()\n",
    "    df.to_csv(file_path)\n",
    "    \n",
    "    s3 = boto3.client('s3', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY)\n",
    "    file_name = os.path.basename(file_path)\n",
    "    try:\n",
    "        s3.upload_file(file_name, bucket_name, file_name)\n",
    "        print(f\"File {file_name} uploaded to {bucket_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading file {file_name} to {bucket_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b61969f6-5971-4cd6-a1c4-17e9363a04de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "# save a local CSV from the notebook\n",
    "def create_download_link(query, title=\"Download CSV file\", filename=\"data.csv\"):\n",
    "    start_time = time.monotonic()\n",
    "    df = query.toPandas()\n",
    "    csv = df.to_csv()\n",
    "    # with open(filename, \"w\", encoding=\"utf-8\") as fout:\n",
    "    #     print(csv, file=fout)\n",
    "    b64 = base64.b64encode(csv.encode())\n",
    "    payload = b64.decode()\n",
    "    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n",
    "    html = html.format(payload=payload, title=title, filename=filename)\n",
    "    display(f\"{time.monotonic() - start_time}s\")\n",
    "    return HTML(html)\n",
    "\n",
    "# read in UNGP S3 data from a range of dates\n",
    "def get_date_list(basepath, start_date, end_date):\n",
    "    start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "    end_date = datetime.datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
    "    delta = end_date - start_date\n",
    "    days = []\n",
    "    for i in range(delta.days + 1):\n",
    "        day = start_date + datetime.timedelta(days=i)\n",
    "        days.append(datetime.datetime.strftime(day, \"%Y-%m-%d\"))\n",
    "    \n",
    "    paths = [basepath + f\"year={day[:4]}/month={day[5:7]}/day={day[8:10]}\" for day in days]\n",
    "    return (paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "406478df-0eaf-4c8e-9887-a49c128d30fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Entry Point of Suez Canal, Egypt\t30.5852 N\t32.2650 E\n",
    "# Exit Point of Suez Canal, Egypt\t29.9636 N\t32.5618 E\n",
    "# Bosphorus,strait,TUR,41.11833286,29.07183305\n",
    "# Suez mid 30.443370,32.355877\n",
    "# geographic locations\n",
    "# locations = pd.read_csv(\"https://github.com/dhopp1-UNCTAD/ais_helper_files/raw/main/geographic_locations.csv\")\n",
    "# locations = pd.read_csv(\"https://raw.githubusercontent.com/UNECE/AIS/master/wpi_12nm_bounding_box_port.csv\")\n",
    "locations = pd.DataFrame({\n",
    "    \"name\": [\n",
    "        \"Suez (EG)\",\n",
    "        \"Bosphorus (TR)\",\n",
    "    ],\n",
    "    \"longitude\": [\n",
    "        32.355877,\n",
    "        29.07183305,\n",
    "    ],\n",
    "    \"latitude\": [\n",
    "        30.443370,\n",
    "        41.11833286,\n",
    "    ],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f38dda5-e5ec-4c20-b288-b4dbeeeb6451",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "locs = locations.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70b1b652-8b80-4276-b76e-1fe7117cdb59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(start_date, end_date, locations, distance_parameter = \"0.05\"):\n",
    "    # distance parameter = 0.01 = 1 kilometer radius\n",
    "    # distance parameter = 0.05 = 5 kilometer radius\n",
    "    # distance parameter = 0.3 = 30 kilometer radius?\n",
    "    \n",
    "    # suez bbox\n",
    "    # latitude min 29.9 max 30.6\n",
    "    # longitude min 32 max 33\n",
    "    bbox_0_lat_min = 29.9\n",
    "    bbox_0_lat_max = 30.6\n",
    "    bbox_0_lon_min = 32\n",
    "    bbox_0_lon_max = 33\n",
    "    \n",
    "    # bosphorus bbox\n",
    "    # latitude min 41 max 41.2\n",
    "    # longitude min 28.95 max 29.2\n",
    "    bbox_1_lat_min = 41\n",
    "    bbox_1_lat_max = 41.2\n",
    "    bbox_1_lon_min = 28.95\n",
    "    bbox_1_lon_max = 29.2\n",
    "\n",
    "    # all geographies in one query\n",
    "    condition_string = \"\"\n",
    "    select_string = \"\"\n",
    "    pos = \"pos\"\n",
    "    for name_i in locations.name:\n",
    "        name_s = name_i.replace('\\'', '')\n",
    "        condition_string += f\"\"\"ST_Contains(ST_Buffer(ST_Point({locations.loc[locations.name == name_i, 'longitude'].values[0]}, {locations.loc[locations.name == name_i, 'latitude'].values[0]}), {distance_parameter}), {pos})\"\"\"\n",
    "        if name_i != locations.name.values[-1]:\n",
    "            condition_string += \" OR \"\n",
    "        if name_i == locations.name.values[0]:\n",
    "            select_string += f\"\"\"CASE WHEN ST_Contains(ST_Buffer(ST_Point({locations.loc[locations.name == name_i, 'longitude'].values[0]}, {locations.loc[locations.name == name_i, 'latitude'].values[0]}), {distance_parameter}), {pos}) THEN '{name_s}' \"\"\"\n",
    "        elif name_i != locations.name.values[-1]:\n",
    "            select_string += f\"\"\"WHEN ST_Contains(ST_Buffer(ST_Point({locations.loc[locations.name == name_i, 'longitude'].values[0]}, {locations.loc[locations.name == name_i, 'latitude'].values[0]}), {distance_parameter}), {pos}) THEN '{name_s}' \"\"\"\n",
    "        else:\n",
    "            select_string += f\"\"\"WHEN ST_Contains(ST_Buffer(ST_Point({locations.loc[locations.name == name_i, 'longitude'].values[0]}, {locations.loc[locations.name == name_i, 'latitude'].values[0]}), {distance_parameter}), {pos}) THEN '{name_s}' \"\"\"\n",
    "            select_string += \"END AS geo_name\"\n",
    "    \n",
    "    # step 1\n",
    "    # read data\n",
    "    basepath = \"s3a://ungp-ais-data-historical-backup/exact-earth-data/transformed/prod/\"\n",
    "    dates = get_date_list(basepath, start_date, end_date)\n",
    "    df = spark.read.parquet(*dates)\n",
    "\n",
    "    # create temp view to be able to use spark SQL\n",
    "    df.createOrReplaceTempView(\"df\")\n",
    "    \n",
    "    # print(spark.sql(\"SELECT * FROM df LIMIT 1\").toPandas().values.tolist())\n",
    "    # print(spark.sql(\"SELECT * FROM df LIMIT 1\").toPandas().columns.tolist())\n",
    "\n",
    "    # adding points and filtering for cargo and tankers\n",
    "    step_01 = spark.sql(f\"\"\"\n",
    "                    SELECT DISTINCT vessel_type, mmsi, date_year, date_month, {select_string} FROM\n",
    "                    (\n",
    "                    SELECT vessel_type, mmsi, date_year, date_month, ST_Point(lon, lat) as pos FROM\n",
    "                    (\n",
    "                        SELECT DISTINCT\n",
    "                            YEAR(dt_pos_utc) as date_year, \n",
    "                            MONTH(dt_pos_utc) AS date_month,\n",
    "                            mmsi, \n",
    "                            vessel_type,\n",
    "                            cast(longitude as Decimal(6,3)) as lon,\n",
    "                            cast(latitude as Decimal(6,3)) as lat\n",
    "                        FROM df\n",
    "                        WHERE vessel_type IN ('Cargo','Tanker')\n",
    "                    ) AS subquery\n",
    "                    WHERE\n",
    "                       (lon >= {bbox_0_lon_min} AND lon <= {bbox_0_lon_max} AND lat >= {bbox_0_lat_min} AND lat <= {bbox_0_lat_max})\n",
    "                    OR (lon >= {bbox_1_lon_min} AND lon <= {bbox_1_lon_max} AND lat >= {bbox_1_lat_min} AND lat <= {bbox_1_lat_max})\n",
    "                    ) AS subsubquery\n",
    "                    WHERE {condition_string}\n",
    "                    \"\"\")\n",
    "    return (step_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5bc267e2-6624-47f2-9b7f-3839552bbedb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# queries for months\n",
    "# start_month = datetime.datetime.strptime(\"2018-12-01\", \"%Y-%m-%d\")\n",
    "# end_month = datetime.datetime.strptime(\"2021-07-01\", \"%Y-%m-%d\")\n",
    "\n",
    "#start_month = datetime.datetime.strptime(\"2021-08-01\", \"%Y-%m-%d\")\n",
    "start_month = datetime.datetime.strptime(\"2021-08-01\", \"%Y-%m-%d\")\n",
    "end_month = datetime.datetime.strptime(\"2021-08-01\", \"%Y-%m-%d\")\n",
    "\n",
    "# start_month = datetime.datetime.strptime(\"2022-11-01\", \"%Y-%m-%d\")\n",
    "# end_month = datetime.datetime.strptime(\"2023-10-01\", \"%Y-%m-%d\")\n",
    "\n",
    "start_dates = []\n",
    "end_dates = []\n",
    "\n",
    "while start_month <= end_month:\n",
    "    start_dates.append(datetime.datetime.strftime(start_month, \"%Y-%m-%d\"))\n",
    "    end_date = min(start_month + relativedelta(months=1) - relativedelta(days = 1), datetime.datetime.today() - relativedelta(days=2)) # minimum between 2 days ago so don't go ahead of where there are actually files\n",
    "    end_dates.append(datetime.datetime.strftime(end_date, \"%Y-%m-%d\"))\n",
    "    start_month = start_month + relativedelta(months=1)\n",
    "\n",
    "date_dict = {f\"{x}\": None for x in start_dates}\n",
    "\n",
    "for i in range(len(start_dates)):\n",
    "    date_dict[f\"{start_dates[i]}\"] = get_data(start_dates[i], end_dates[i], locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "918d4b29-1a2b-4e02-9fde-371c508f5b07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = \"strait_ships\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c8b20e8f-c819-4986-bcb7-9a461a95a21d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "quick = []\n",
    "quick += ['2021-07-01','2021-08-01','2021-09-01'] # These dates give an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b33ad3a4-af04-4dc1-a52e-bfd7b958011c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for date_str in quick:\n",
    "#    display((prefix, date_str))\n",
    "#    #upload_file_to_s3(date_dict[date_str], f\"{prefix}_{date_str}.csv\", 'un-mercatorians')\n",
    "#    display(create_download_link(date_dict[date_str], filename=f\"{prefix}_{date_str}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "888fd3b1-9967-49d1-a998-158547c99814",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('strait_ships', '2021-08-01')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1169.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 26 in stage 476.0 failed 4 times, most recent failure: Lost task 26.3 in stage 476.0 (TID 36895, 192.168.37.204, executor 6): scala.MatchError: null\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Point.eval(Constructors.scala:230)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Contains.eval(Predicates.scala:47)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\n\tat org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3(basicPhysicalOperators.scala:233)\n\tat org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3$adapted(basicPhysicalOperators.scala:232)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: scala.MatchError: null\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Point.eval(Constructors.scala:230)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Contains.eval(Predicates.scala:47)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\n\tat org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3(basicPhysicalOperators.scala:233)\n\tat org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3$adapted(basicPhysicalOperators.scala:232)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark/work-dir/launch_ipykernel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mupload_file_to_s3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{prefix}_{date_str}.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'un-mercatorians'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m#display(create_download_link(query, filename=f\"{prefix}_{date_str}.csv\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/work-dir/launch_ipykernel.py\u001b[0m in \u001b[0;36mupload_file_to_s3\u001b[0;34m(query, file_path, bucket_name)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupload_file_to_s3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \"\"\"\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1169.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 26 in stage 476.0 failed 4 times, most recent failure: Lost task 26.3 in stage 476.0 (TID 36895, 192.168.37.204, executor 6): scala.MatchError: null\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Point.eval(Constructors.scala:230)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Contains.eval(Predicates.scala:47)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\n\tat org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3(basicPhysicalOperators.scala:233)\n\tat org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3$adapted(basicPhysicalOperators.scala:232)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: scala.MatchError: null\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Point.eval(Constructors.scala:230)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Contains.eval(Predicates.scala:47)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\n\tat org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3(basicPhysicalOperators.scala:233)\n\tat org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3$adapted(basicPhysicalOperators.scala:232)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "for date_str, query in date_dict.items():\n",
    "    if date_str in quick:\n",
    "        continue\n",
    "    display((prefix, date_str))\n",
    "    upload_file_to_s3(query, f\"{prefix}_{date_str}.csv\", 'un-mercatorians')\n",
    "    #display(create_download_link(query, filename=f\"{prefix}_{date_str}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072d4bd8-1ef3-4bb4-82b1-887fe56be35d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#lcsv = locs[[\"name\", \"longitude\", \"latitude\"]].to_csv()\n",
    "#lb64 = base64.b64encode(lcsv.encode())\n",
    "#lpayload = lb64.decode()\n",
    "#lhtml = f'<a download=\"locs.csv\" href=\"data:text/csv;base64,{lpayload}\" target=\"_blank\">locs_straights.csv</a>'\n",
    "#display(HTML(lhtml))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Config template extra",
   "language": "python3",
   "name": "extra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
